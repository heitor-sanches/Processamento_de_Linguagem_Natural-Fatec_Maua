{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.\tIntrodução:\n",
        "Este roteiro de prática tem como objetivo guiar o aluno na construção de duas implementações de redes neurais para Processamento de Linguagem Natural (PLN): RNNs Simples para previsão da próxima palavra e LSTMs para classificação de sentimentos. O roteiro abrange as etapas cruciais para cada implementação, desde a preparação dos dados até o treinamento e avaliação dos modelos. O propósito é que o aluno compreenda e aplique os conceitos de redes neurais para resolver problemas de PLN de forma eficaz.\n"
      ],
      "metadata": {
        "id": "Z8sjP58DvVQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.\tObjetivos:\n",
        "•\tImplementar uma Rede Neural Recorrente (RNN) simples em Python para prever a próxima palavra em uma sequência de texto, utilizando a biblioteca TensorFlow/Keras.\n",
        "•\tImplementar uma Rede Long Short-Term Memory (LSTM) em Python para classificar o sentimento de frases como \"positivo\" ou \"negativo\", utilizando a biblioteca TensorFlow/Keras.\n"
      ],
      "metadata": {
        "id": "oC6dgtI1vZND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.\tProjeto a Ser Desenvolvido:\n",
        "##3.1\tFuncionalidades:\n",
        "Os programas a serem desenvolvido:\n",
        "\n",
        "•\tPrevisão da Próxima Palavra: Implementar uma RNN simples para prever a próxima palavra em uma sequência de texto.\n",
        "\n",
        "•\tClassificação de Sentimentos: Implementar uma LSTM para classificar o sentimento de frases como \"positivo\" ou \"negativo\".\n",
        "##3.2\tEtapas de Desenvolvimento (Fluxo do Programa):\n",
        "###3.2.1\tImplementação 1: RNNs Simples para Previsão da Próxima Palavra\n",
        "•\tEtapa 1: Preparação dos dados: Coletar e pré-processar um corpus de texto para treinamento.\n",
        "\n",
        "•\tEtapa 2: Construção do modelo RNN:  Definir a arquitetura da RNN simples.\n",
        "\n",
        "•\tEtapa 3: Treinamento do modelo: Treinar a RNN usando o corpus preparado.\n",
        "\n",
        "•\tEtapa 4: Avaliação do modelo: Avaliar o desempenho da RNN na previsão da próxima palavra.\n",
        "\n",
        "•\tEtapa 5: Teste do modelo: Testar o modelo com novas frases.\n",
        "###3.2.2\tImplementação 2: LSTMs para Classificação de Sentimentos\n",
        "•\tEtapa 1: Preparação dos dados: Coletar e pré-processar um conjunto de dados de frases rotuladas (positivo/negativo).\n",
        "\n",
        "•\tEtapa 2: Construção do modelo LSTM: Definir a arquitetura da LSTM.\n",
        "\n",
        "•\tEtapa 3: Treinamento do modelo: Treinar a LSTM usando os dados rotulados.\n",
        "\n",
        "•\tEtapa 4: Avaliação do modelo: Avaliar o desempenho da LSTM na classificação de sentimentos.\n",
        "\n",
        "•\tEtapa 5: Teste do modelo: Testar o modelo com novas frases.\n"
      ],
      "metadata": {
        "id": "85DAJRSRvgrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.\tProcedimentos Detalhados:"
      ],
      "metadata": {
        "id": "k5LGhsvUwJqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1\tImplementação 1: Modelo de Rede Neural de Recorrência"
      ],
      "metadata": {
        "id": "VEUzKd8nwMvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1: Configuração do Ambiente no Google Colab"
      ],
      "metadata": {
        "id": "1rmeGRZnw1r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Gbxkouws6n",
        "outputId": "b5f02412-7945-48ab-f98e-6741a35331ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explicação\n",
        "\n",
        "*   numpy: Para operações numéricas\n",
        "*   tensorflow.keras: A API de alto nível para construir e treinar modelos de deep learning\n",
        "*   Embedding: Camada que transforma palavras (índices numéricos) em vetores densos.\n",
        "*   SimpleRNN: A camada de Rede Neural Recorrente mais básica.\n",
        "*   Dense: Camada neural comum (fully connected layer).\n",
        "*   Tokenizer: Para converter texto em sequências de números.\n",
        "*   pad_sequences: Para garantir que todas as sequências de entrada tenham o mesmo comprimento."
      ],
      "metadata": {
        "id": "u6w_9HrGxKso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 2: Preparação do Conjunto de Dados"
      ],
      "metadata": {
        "id": "BDZVO0K3xkTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definir o conjunto de treinamento de dados (frases e rótulos)\n",
        "textos_treinamento = [\n",
        "    \"eu gosto de programar em python\",\n",
        "    \"python é uma linguagem poderosa\",\n",
        "    \"programação divertida em python\",\n",
        "    \"aprenda python e seja feliz\",\n",
        "    \"gosto de aprender coisas novas\"\n",
        "]\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")\n",
        "\n",
        "# 2. Inicializar o tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(textos_treinamento)\n",
        "\n",
        "# 3. Converter textos para sequências de números\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "print(f\"Vocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas dos textos: {sequencias}\")\n",
        "\n",
        "# 4. Calcular o tamanho do vocabulário (+1 para incluir o 0 de padding)\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")\n",
        "\n",
        "# 5. Preparar Entradas (X) e Saídas (Y) para a previsão da próxima palavra\n",
        "# A entrada (X) será uma sequência de palavras, e a saída (Y) será a palavra seguinte.\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"Comprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "entradas_X = []\n",
        "saidas_Y = []\n",
        "\n",
        "for seq in sequencias:\n",
        "    for i in range(1, len(seq)):\n",
        "        entradas_X.append(seq[:i]) # A sequência até a palavra atual\n",
        "        saidas_Y.append(seq[i])   # A próxima palavra\n",
        "\n",
        "print(f\"Exemplo de entradas X (parcial): {entradas_X[:5]}\")\n",
        "print(f\"Exemplo de saídas Y (parcial): {saidas_Y[:5]}\")\n",
        "\n",
        "# 6. Padronizar o comprimento das sequências de entrada\n",
        "# Todas as sequências de entrada precisam ter o mesmo comprimento para a RNN.\n",
        "# Usamos max_len-1 porque a saída, que é a última palavra, pad_sequences sempre terá 1 palavra a menos.\n",
        "entradas_X_padded = pad_sequences(entradas_X, maxlen=max_comprimento - 1, padding='pre')\n",
        "print(f\"Formato final das entradas (X): {entradas_X_padded.shape}\")\n",
        "\n",
        "# 7. Converter as saídas para o formato one-hot encoding\n",
        "# Isso é necessário para a camada de saída do RNN (softmax)\n",
        "saidas_Y_one_hot = tf.keras.utils.to_categorical(saidas_Y, num_classes=total_palavras)\n",
        "print(f\"Formato final das saídas (Y): {saidas_Y_one_hot.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFYUtc2uxnWu",
        "outputId": "10e0e023-49f4-4f37-a283-608cc8e5912e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programação divertida em python', 'aprenda python e seja feliz', 'gosto de aprender coisas novas']\n",
            "Vocabulário (palavra: índice): {'python': 1, 'gosto': 2, 'de': 3, 'em': 4, 'eu': 5, 'programar': 6, 'é': 7, 'uma': 8, 'linguagem': 9, 'poderosa': 10, 'programação': 11, 'divertida': 12, 'aprenda': 13, 'e': 14, 'seja': 15, 'feliz': 16, 'aprender': 17, 'coisas': 18, 'novas': 19}\n",
            "Sequências numéricas dos textos: [[5, 2, 3, 6, 4, 1], [1, 7, 8, 9, 10], [11, 12, 4, 1], [13, 1, 14, 15, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas X (parcial): [[5], [5, 2], [5, 2, 3], [5, 2, 3, 6], [5, 2, 3, 6, 4]]\n",
            "Exemplo de saídas Y (parcial): [2, 3, 6, 4, 1]\n",
            "Formato final das entradas (X): (20, 5)\n",
            "Formato final das saídas (Y): (20, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explicação:\n",
        "*   Geracão de Pares (Sequência --> Próxima Palavra) para treinar a RNN a prever a próxima palavra, transformamos cada frase em múltiplos pares de (sequência parcial, próxima palavra).\n",
        "*   pad_sequences: É vital. Como as sequências parciais têm comprimentos variados, pad_sequences preenche (com zeros, por padrão) as sequências mais curtas para que todas tenham o mesmo maxlen. O 'padding='pre' significa que os zeros são adicionados no início.\n",
        "*   to_categorical: A camada de saída da RNN (com softmax) produz uma probablidade para cada palavra no vocabulário. to_categorical converte o índice da palavra real em um vetor, onde apenas a posição da palavra correta é 1 e o resto é 0, chamado de \"one-hot encoding\"."
      ],
      "metadata": {
        "id": "GRTKakf4xv7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 3: Construção do Modelo RNN"
      ],
      "metadata": {
        "id": "UWHxNRvLx3ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definindo o modelo\n",
        "modelo_rnn = Sequential()\n",
        "\n",
        "# 2. Camada de Embedding:\n",
        "# total_palavras: tamanho do vocabulário\n",
        "# 10: dimensão do vetor de embedding (quantas características queremos para cada palavra)\n",
        "# input_length: comprimento padronizado das sequências de entrada (maxlen - 1)\n",
        "modelo_rnn.add(Embedding(total_palavras, 10, input_length=entradas_X_padded.shape[1]))\n",
        "\n",
        "# 3. Camada SimpleRNN:\n",
        "# 32: número de unidades (neurônios) na camada recorrente. Este é o tamanho do estado oculto.\n",
        "modelo_rnn.add(SimpleRNN(32))\n",
        "\n",
        "# 4. Camada Densa de Saída:\n",
        "# total_palavras: número de neurônios de saída (um para cada palavra no vocabulário)\n",
        "# activation='softmax': função de ativação para probabilidade (soma 1 para todas as palavras)\n",
        "modelo_rnn.add(Dense(total_palavras, activation='softmax'))\n",
        "\n",
        "# 5. Compilar o modelo\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. Exibir um resumo da arquitetura do modelo\n",
        "modelo_rnn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "7Q29Sr-lx5un",
        "outputId": "38ba9979-b26f-4a35-ae91-11d0b1155dea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explicação da Arquitetura:\n",
        "\n",
        "* Embedding Layer: Essencial em PLN. Ela mapeia cada palavra (representada por seu índice numérico) para um vetor denso de embedding. Este vetor captura relações semânticas entre as palavras. Por exemplo, palavras com significados semelhantes estarão \"próximas\" no espaço de embedding.\n",
        "* SimpleRNN Layer: Esta é a camada recorrente. Ela recebe as sequências de embeddings e processa-as passo a passo. O 32 indica a dimensão do vetor de estado oculto (ou seja, a 'memória' que a RNN carrega ao longo do tempo).\n",
        "* Dense (Output) Layer: Esta camada final recebe o estado oculto final da SimpleRNN e o transforma em um vetor de probabilidades, onde cada posição corresponde a uma palavra do vocabulário. A função softmax garante que a soma dessas probabilidades seja 1."
      ],
      "metadata": {
        "id": "OUSDKn3iyAHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 4: Treinamento do Modelo"
      ],
      "metadata": {
        "id": "jy1MCf2hyDS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Treinando o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "modelo_rnn.fit(entradas_X_padded, saidas_Y_one_hot, epochs=100, verbose=1)\n",
        "# epochs: quantas vezes o modelo verá todo o conjunto de dados\n",
        "# verbose: 1 para mostrar o progresso do treinamento\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z2h5H-FyGXD",
        "outputId": "cd544d7b-d156-45c1-d1af-e9f0802a81d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 2.9917\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 2.9838\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 2.9759\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1000 - loss: 2.9680\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1000 - loss: 2.9599\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1500 - loss: 2.9518\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2000 - loss: 2.9434\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 2.9349\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2000 - loss: 2.9262\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2500 - loss: 2.9173\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 2.9080\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 2.8985\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 2.8886\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 2.8784\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.2500 - loss: 2.8678\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 2.8568\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2000 - loss: 2.8453\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2000 - loss: 2.8334\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 2.8211\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2000 - loss: 2.8082\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2000 - loss: 2.7949\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.7810\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2000 - loss: 2.7667\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2000 - loss: 2.7518\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2000 - loss: 2.7365\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2000 - loss: 2.7206\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2000 - loss: 2.7043\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 2.6876\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2000 - loss: 2.6705\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2000 - loss: 2.6529\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2000 - loss: 2.6350\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2000 - loss: 2.6167\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.5982\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2000 - loss: 2.5793\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2000 - loss: 2.5601\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 2.5407\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 2.5210\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 2.5009\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 2.4806\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3000 - loss: 2.4600\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3000 - loss: 2.4390\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3000 - loss: 2.4178\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3000 - loss: 2.3962\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.3000 - loss: 2.3744\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3000 - loss: 2.3523\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.3500 - loss: 2.3300\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3500 - loss: 2.3076\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3500 - loss: 2.2850\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3500 - loss: 2.2623\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3500 - loss: 2.2394\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4000 - loss: 2.2164\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4500 - loss: 2.1932\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4500 - loss: 2.1698\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4500 - loss: 2.1461\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4500 - loss: 2.1221\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 2.0978\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5500 - loss: 2.0733\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5500 - loss: 2.0485\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5500 - loss: 2.0236\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5500 - loss: 1.9985\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5500 - loss: 1.9733\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6000 - loss: 1.9481\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 1.9229\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6500 - loss: 1.8977\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6000 - loss: 1.8726\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6500 - loss: 1.8477\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7000 - loss: 1.8228\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7000 - loss: 1.7980\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.7734\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7000 - loss: 1.7489\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7000 - loss: 1.7245\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.7002\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7000 - loss: 1.6761\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.6521\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7000 - loss: 1.6283\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.6046\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7000 - loss: 1.5811\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7000 - loss: 1.5578\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.5346\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7000 - loss: 1.5116\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7000 - loss: 1.4888\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7000 - loss: 1.4663\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7000 - loss: 1.4439\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7000 - loss: 1.4217\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7000 - loss: 1.3997\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.3780\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.3564\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7000 - loss: 1.3351\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7000 - loss: 1.3140\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7000 - loss: 1.2931\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7000 - loss: 1.2725\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.2520\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7000 - loss: 1.2319\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7000 - loss: 1.2119\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7000 - loss: 1.1922\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7000 - loss: 1.1727\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7000 - loss: 1.1534\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7500 - loss: 1.1344\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 1.1156\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8500 - loss: 1.0971\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 5: Usar o Modelo para Previsão"
      ],
      "metadata": {
        "id": "2sOHM4-nyeJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "    \"\"\"\n",
        "    Prevê a próxima palavra dado um texto base.\n",
        "    \"\"\"\n",
        "    # Converter o texto base para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([texto_base])[0]\n",
        "\n",
        "    # Padronizar o comprimento da sequência de entrada (precisa ter o mesmo formato que o treinamento)\n",
        "    # Atenção: max_seq_len deve ser o comprimento que as \"entradas\" foram pad_sequences\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "    # Fazer a previsão\n",
        "    previsao_probabilidades = modelo.predict(sequencia_padded, verbose=0)[0]\n",
        "\n",
        "    # Obter o índice da palavra com a maior probabilidade\n",
        "    indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "    # Converter o índice de volta para a palavra\n",
        "    for palavra, indice in tokenizer.word_index.items():\n",
        "        if indice == indice_palavra_prevista:\n",
        "            return palavra\n",
        "    return None # Caso a palavra não seja encontrada (improvável com o vocabulário ajustado)\n",
        "\n",
        "# Comprimento de entrada esperado pelo modelo\n",
        "# entradas_X_padded.shape[1] É o maxlen que usamos no pad_sequences para X\n",
        "comprimento_entrada_modelo = entradas_X_padded.shape[1]\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo RNN ---\")\n",
        "\n",
        "texto_teste_1 = \"eu gosto de\"\n",
        "proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_1)\n",
        "print(f\"Texto: '{texto_teste_1}' -> Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "texto_teste_2 = \"python é uma\"\n",
        "proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_2)\n",
        "print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "texto_teste_3 = \"programar é divertido\"\n",
        "proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_3)\n",
        "print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "texto_teste_4 = \"aprenda python e\"\n",
        "proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_4)\n",
        "print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "# Exemplo com palavra fora do vocabulário (ou sequência que o modelo nunca viu antes)\n",
        "texto_teste_5 = \"o sol brilha no\" # Palavras \"sol\" e \"brilha\" não estão no vocabulário\n",
        "proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_5)\n",
        "print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}' (Pode ser inesperada devido a palavras desconhecidas)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqZMOYodyfTy",
        "outputId": "ba56ec5b-76e9-466c-8421-75bee619844f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo RNN ---\n",
            "Texto: 'eu gosto de' -> Próxima palavra prevista: 'programar'\n",
            "Texto: 'python é uma' -> Próxima palavra prevista: 'linguagem'\n",
            "Texto: 'programar é divertido' -> Próxima palavra prevista: 'python'\n",
            "Texto: 'aprenda python e' -> Próxima palavra prevista: 'seja'\n",
            "Texto: 'o sol brilha no' -> Próxima palavra prevista: 'python' (Pode ser inesperada devido a palavras desconhecidas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação 2: Modelo de Rede Neural Rede Long Short-Term Memory"
      ],
      "metadata": {
        "id": "Tg9tX48Fy6ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação 2: Modelo de Rede Neural Rede Long Short-Term Memory"
      ],
      "metadata": {
        "id": "woOP9OZuzGY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsCW4NLUzdzv",
        "outputId": "6bb06091-f2f1-4091-cc3b-5563d3d809d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###O que tem de novo aqui?\n",
        "* LSTM: A camada de Rede Long Short-Term Memory.\n",
        "* train_test_split (sklearn): Para dividir o dataset em conjuntos de treinamento e teste.\n",
        "* classification_report, confusion_matrix (sklearn): Para avaliar o desempenho do modelo.\n",
        "* matplotlib.pyplot, seaborn: Para visualização dos resultados."
      ],
      "metadata": {
        "id": "_R93ttpPzmuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 2: Preparação do Conjunto de Dados (Frases e Rótulos) para análise de sentimentos"
      ],
      "metadata": {
        "id": "0wT9cZOJzpvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definir o conjunto de dados (frases e rótulos)\n",
        "dados_sentimento = [\n",
        "    (\"Eu gostei muito do filme, divertido\", \"positivo\"),\n",
        "    (\"Que adorei o livro, muito bom\", \"positivo\"),\n",
        "    (\"Gostei muito da atuação dos atores\", \"positivo\"),\n",
        "    (\"O roteiro é fraco e chato\", \"negativo\"),\n",
        "    (\"Não recomendo este último produto\", \"negativo\"),\n",
        "    (\"Uma perda de tempo horrível\", \"negativo\"),\n",
        "    (\"Ótimo trabalho, parabéns\", \"positivo\"),\n",
        "    (\"Terrível experiência, nunca mais\", \"negativo\"),\n",
        "    (\"Excelente serviço, muito eficiente\", \"positivo\"),\n",
        "    (\"Que decepção, muito ruim\", \"negativo\"),\n",
        "    (\"Aprendizado de máquina é fascinante\", \"positivo\"),\n",
        "    (\"Filme foi cansativo, sem graça\", \"negativo\"),\n",
        "    (\"Este software travou várias vezes\", \"negativo\"),\n",
        "    (\"A interface é confusa e difícil\", \"negativo\"),\n",
        "    (\"O aplicativo é super útil e rápido\", \"positivo\"),\n",
        "]\n",
        "\n",
        "textos = [dado[0] for dado in dados_sentimento]\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]\n",
        "\n",
        "print(f\"Total de frases: {len(textos)}\")\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")\n",
        "print(f\"Exemplo de sentimentos: {sentimentos[:3]}\")\n",
        "\n",
        "# 2. Mapear Sentimentos para Números: converter \"positivo\" e \"negativo\" para 0 e 1.\n",
        "mapeamento_sentimento = {'negativo': 0, 'positivo': 1}\n",
        "rotulos_numericos = np.array([mapeamento_sentimento[s] for s in sentimentos])\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n",
        "\n",
        "# 3. Tokenização de Texto\n",
        "tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\") # oov_token para palavras desconhecidas\n",
        "tokenizer.fit_on_texts(textos)\n",
        "sequencias_numericas = tokenizer.texts_to_sequences(textos)\n",
        "\n",
        "# +1 para o 0 de padding/OOV (out of vocabulary)\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1\n",
        "print(f\"\\nVocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas das frases: {sequencias_numericas}\")\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# 4. Padronizar o comprimento das sequências\n",
        "# Encontrar o comprimento da frase mais longa para padronizar\n",
        "max_len = max(len(s) for s in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_len}\")\n",
        "\n",
        "# 'post' para adicionar zeros no final\n",
        "sequencias_padded = pad_sequences(sequencias_numericas, maxlen=max_len, padding='post')\n",
        "print(f\"Sequências após padding: {sequencias_padded}\")\n",
        "\n",
        "# 5. Dividir os dados em conjuntos de treinamento e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "    sequencias_padded, rotulos_numericos, test_size=0.2, random_state=42, stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de y_teste: {y_teste.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID6ogPIzzszd",
        "outputId": "901be2df-6201-4f2b-ccc4-94a6eddda0f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['Eu gostei muito do filme, divertido', 'Que adorei o livro, muito bom', 'Gostei muito da atuação dos atores']\n",
            "Exemplo de sentimentos: ['positivo', 'positivo', 'positivo']\n",
            "\n",
            "Sentimentos mapeados para números: [1 1 1 0 0 0 1 0 1 0 1 0 0 0 1]\n",
            "\n",
            "Vocabulário (palavra: índice): {'<unk>': 1, 'muito': 2, 'é': 3, 'o': 4, 'e': 5, 'gostei': 6, 'filme': 7, 'que': 8, 'este': 9, 'de': 10, 'eu': 11, 'do': 12, 'divertido': 13, 'adorei': 14, 'livro': 15, 'bom': 16, 'da': 17, 'atuação': 18, 'dos': 19, 'atores': 20, 'roteiro': 21, 'fraco': 22, 'chato': 23, 'não': 24, 'recomendo': 25, 'último': 26, 'produto': 27, 'uma': 28, 'perda': 29, 'tempo': 30, 'horrível': 31, 'ótimo': 32, 'trabalho': 33, 'parabéns': 34, 'terrível': 35, 'experiência': 36, 'nunca': 37, 'mais': 38, 'excelente': 39, 'serviço': 40, 'eficiente': 41, 'decepção': 42, 'ruim': 43, 'aprendizado': 44, 'máquina': 45, 'fascinante': 46, 'foi': 47, 'cansativo': 48, 'sem': 49, 'graça': 50, 'software': 51, 'travou': 52, 'várias': 53, 'vezes': 54, 'a': 55, 'interface': 56, 'confusa': 57, 'difícil': 58, 'aplicativo': 59, 'super': 60, 'útil': 61, 'rápido': 62}\n",
            "Sequências numéricas das frases: [[11, 6, 2, 12, 7, 13], [8, 14, 4, 15, 2, 16], [6, 2, 17, 18, 19, 20], [4, 21, 3, 22, 5, 23], [24, 25, 9, 26, 27], [28, 29, 10, 30, 31], [32, 33, 34], [35, 36, 37, 38], [39, 40, 2, 41], [8, 42, 2, 43], [44, 10, 45, 3, 46], [7, 47, 48, 49, 50], [9, 51, 52, 53, 54], [55, 56, 3, 57, 5, 58], [4, 59, 3, 60, 61, 5, 62]]\n",
            "Tamanho total do vocabulário: 63\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "Sequências após padding: [[11  6  2 12  7 13  0]\n",
            " [ 8 14  4 15  2 16  0]\n",
            " [ 6  2 17 18 19 20  0]\n",
            " [ 4 21  3 22  5 23  0]\n",
            " [24 25  9 26 27  0  0]\n",
            " [28 29 10 30 31  0  0]\n",
            " [32 33 34  0  0  0  0]\n",
            " [35 36 37 38  0  0  0]\n",
            " [39 40  2 41  0  0  0]\n",
            " [ 8 42  2 43  0  0  0]\n",
            " [44 10 45  3 46  0  0]\n",
            " [ 7 47 48 49 50  0  0]\n",
            " [ 9 51 52 53 54  0  0]\n",
            " [55 56  3 57  5 58  0]\n",
            " [ 4 59  3 60 61  5 62]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "Shape de X_teste: (3, 7)\n",
            "Shape de y_treino: (12,)\n",
            "Shape de y_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 3: Construção do Modelo LSTM"
      ],
      "metadata": {
        "id": "oUfsBKeez2dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definir a arquitetura do modelo LSTM\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# 2. Camada de Embedding: Converte os índices numéricos das palavras em vetores densos.\n",
        "# total_palavras_vocab: tamanho do vocabulário\n",
        "# 50: dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequências (max_len)\n",
        "modelo_lstm.add(Embedding(total_palavras_vocab, 50, input_length=max_len))\n",
        "\n",
        "# 3. Camada LSTM:\n",
        "# 64: número de unidades (neurônios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropout: Um tipo de regularização para evitar overfitting (descarta aleatoriamente neurônios durante o treinamento).\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# 4. Camada Densa de Saída:\n",
        "# 1: Um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': Função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 5. Compilar o modelo\n",
        "modelo_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "pMFSglkPz472",
        "outputId": "beffaa99-5788-454d-b6ed-363d48ad7fb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 4: Treinamento e Avaliação do Modelo"
      ],
      "metadata": {
        "id": "oaxu4_240AZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=50, # Reduzi para 50 epochs para um treinamento mais rápido no exemplo. Pode ser aumentado.\n",
        "    batch_size=2, # Pequeno batch_size para dataset pequeno.\n",
        "    validation_split=0.1, # Usar 10% do treino para validação\n",
        "    verbose=1\n",
        ")\n",
        "# epochs: número de vezes que o modelo verá todo o conjunto de treinamento.\n",
        "# batch_size: número de amostras por atualização de gradiente.\n",
        "# validation_split: % dos dados de treino usados para validação durante o treinamento (opcional, mas bom para monitorar overfitting).\n",
        "print(\"Treinamento concluído!\")\n",
        "\n",
        "# 2. Avaliar o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "print(f\"Acurácia do modelo no conjunto de teste: {acuracia*100:.2f}%\")\n",
        "print(f\"Perda do modelo no conjunto de teste: {perda:.4f}\")\n",
        "\n",
        "# 3. Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int) # Converter probabilidades para 0 ou 1\n",
        "\n",
        "print(\"\\n--- Relatório de Classificação ---\")\n",
        "print(classification_report(y_teste, y_pred_classes, target_names=['negativo', 'positivo']))\n",
        "\n",
        "print(\"\\n--- Matriz de Confusão ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DzasVMUX0Bzk",
        "outputId": "8587bb6c-a6f9-409b-f5a5-efb850eb2ac5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - accuracy: 0.4389 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6936\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6778 - loss: 0.6903 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7931 - loss: 0.6906 - val_accuracy: 0.5000 - val_loss: 0.6954\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9181 - loss: 0.6834 - val_accuracy: 0.5000 - val_loss: 0.6965\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9458 - loss: 0.6742 - val_accuracy: 0.5000 - val_loss: 0.6974\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8764 - loss: 0.6662 - val_accuracy: 0.5000 - val_loss: 0.6988\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9667 - loss: 0.6438 - val_accuracy: 0.5000 - val_loss: 0.6996\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8764 - loss: 0.6071 - val_accuracy: 0.5000 - val_loss: 0.6985\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.5608 - val_accuracy: 0.5000 - val_loss: 0.6991\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8764 - loss: 0.4595 - val_accuracy: 0.5000 - val_loss: 0.6986\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2969 - val_accuracy: 0.5000 - val_loss: 0.6952\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1537 - val_accuracy: 0.5000 - val_loss: 0.7093\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0695 - val_accuracy: 0.5000 - val_loss: 0.7707\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0365 - val_accuracy: 0.5000 - val_loss: 0.8593\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0197 - val_accuracy: 0.5000 - val_loss: 0.9642\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.5000 - val_loss: 1.0632\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 0.5000 - val_loss: 1.1542\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.5000 - val_loss: 1.2310\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.5000 - val_loss: 1.2842\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.5000 - val_loss: 1.3288\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.5000 - val_loss: 1.3591\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.5000 - val_loss: 1.3783\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.5000 - val_loss: 1.3960\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.5000 - val_loss: 1.4180\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.5000 - val_loss: 1.4359\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.5000 - val_loss: 1.4488\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.5000 - val_loss: 1.4649\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.5000 - val_loss: 1.4796\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.5000 - val_loss: 1.4957\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.5000 - val_loss: 1.5123\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.5000 - val_loss: 1.5290\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.5000 - val_loss: 1.5462\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.5000 - val_loss: 1.5620\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.5000 - val_loss: 1.5887\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 7.9259e-04 - val_accuracy: 0.5000 - val_loss: 1.6090\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 9.7360e-04 - val_accuracy: 0.5000 - val_loss: 1.6240\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.5000 - val_loss: 1.6383\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 9.6751e-04 - val_accuracy: 0.5000 - val_loss: 1.6518\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.5000 - val_loss: 1.6643\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.5000 - val_loss: 1.6728\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.5000 - val_loss: 1.6795\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.0493e-04 - val_accuracy: 0.5000 - val_loss: 1.6871\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.5000 - val_loss: 1.6920\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 7.5901e-04 - val_accuracy: 0.5000 - val_loss: 1.6991\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 8.8322e-04 - val_accuracy: 0.5000 - val_loss: 1.7070\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 8.1202e-04 - val_accuracy: 0.5000 - val_loss: 1.7152\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3512e-04 - val_accuracy: 0.5000 - val_loss: 1.7233\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.2419e-04 - val_accuracy: 0.5000 - val_loss: 1.7323\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.2546e-04 - val_accuracy: 0.5000 - val_loss: 1.7376\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3361e-04 - val_accuracy: 0.5000 - val_loss: 1.7417\n",
            "Treinamento concluído!\n",
            "Acurácia do modelo no conjunto de teste: 66.67%\n",
            "Perda do modelo no conjunto de teste: 0.4431\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step\n",
            "\n",
            "--- Relatório de Classificação ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.67      1.00      0.80         2\n",
            "    positivo       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.33      0.50      0.40         3\n",
            "weighted avg       0.44      0.67      0.53         3\n",
            "\n",
            "\n",
            "--- Matriz de Confusão ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 5: Testar o Modelo com Novas Frases"
      ],
      "metadata": {
        "id": "NvbubSlR0QIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento):\n",
        "    \"\"\"\n",
        "    Prevê o sentimento de uma nova frase.\n",
        "    \"\"\"\n",
        "    # Converter a frase para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])[0]\n",
        "\n",
        "    # Se a frase tem palavras desconhecidas, o tokenizer pode retornar uma lista vazia ou valores 0\n",
        "    if not sequencia_numerica:\n",
        "        print(f\"Aviso: A frase '{frase_nova}' contém palavras desconhecidas.\")\n",
        "        return \"Desconhecido ou Outra indicação\" # ou retornar um valor padrão, ou levantar um erro\n",
        "\n",
        "    # Padronizar o comprimento da sequência de entrada\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='post')\n",
        "\n",
        "    # Fazer a previsão (probabilidade)\n",
        "    probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0)[0][0]\n",
        "\n",
        "    # Inverter o mapeamento para obter o nome do sentimento\n",
        "    mapeamento_inverso = {v: k for k, v in mapeamento_sentimento.items()}\n",
        "\n",
        "    # Classificar com base no limiar de 0.5\n",
        "    if probabilidade_positiva > 0.5:\n",
        "        return mapeamento_inverso[1] # 'positivo'\n",
        "    else:\n",
        "        return mapeamento_inverso[0] # 'negativo'\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo LSTM com Novas Frases ---\")\n",
        "\n",
        "frase_nova_1 = \"gostei muito do filme, excelente!\"\n",
        "sentimento_1 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_1, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\")\n",
        "\n",
        "frase_nova_2 = \"odiei o livro, muito entediante\"\n",
        "sentimento_2 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_2, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_2}' -> Sentimento previsto: '{sentimento_2}'\")\n",
        "\n",
        "frase_nova_3 = \"a aula de pln é ótima\"\n",
        "sentimento_3 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_3, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_3}' -> Sentimento previsto: '{sentimento_3}'\")\n",
        "\n",
        "frase_nova_4 = \"o atendimento foi péssimo\"\n",
        "sentimento_4 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_4, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_4}' -> Sentimento previsto: '{sentimento_4}'\")\n",
        "\n",
        "frase_nova_5 = \"esse produto não vale a pena, é caro\"\n",
        "sentimento_5 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_5, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_5}' -> Sentimento previsto: '{sentimento_5}'\")\n",
        "\n",
        "frase_nova_6 = \"o filme é legal\" # Frase curta e ambígua para um modelo pequeno\n",
        "sentimento_6 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_6, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_6}' -> Sentimento previsto: '{sentimento_6}'\")\n",
        "\n",
        "frase_nova_7 = \"isso é horrível, que tristeza\"\n",
        "sentimento_7 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_7, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_7}' -> Sentimento previsto: '{sentimento_7}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msM3MlNx0THv",
        "outputId": "f07389ea-3b7b-4aff-b0c9-aefdf003c93e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo LSTM com Novas Frases ---\n",
            "Frase: 'gostei muito do filme, excelente!' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'odiei o livro, muito entediante' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'a aula de pln é ótima' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'o atendimento foi péssimo' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'esse produto não vale a pena, é caro' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'o filme é legal' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'isso é horrível, que tristeza' -> Sentimento previsto: 'negativo'\n"
          ]
        }
      ]
    }
  ]
}